# Vision-Language
[A Dive into Vision-Language Models](https://huggingface.co/blog/vision_language_pretraining)

- **Contrastive Learning:** Aligning images and texts to a joint feature space in a contrastive manner
- **PrefixLM:** Jointly learning image and text embeddings by using images as a prefix to a language model
- **Multi-modal Fusing with Cross Attention:** Fusing visual information into layers of a language model with a cross-attention mechanism
- **MLM / ITM:** Aligning parts of images with text with masked-language modeling and image-text matching objectives
- **No Training:** Using stand-alone vision and language models via iterative optimization

[Is CLIP still state of the art - or what other text encoder is used? : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/19d6h7w/is_clip_still_state_of_the_art_or_what_other_text/)