# Vision-Language
- **Contrastive Learning:** Aligning images and texts to a joint feature space in a contrastive manner
- **PrefixLM:** Jointly learning image and text embeddings by using images as a prefix to a language model
- **Multi-modal Fusing with Cross Attention:** Fusing visual information into layers of a language model with a cross-attention mechanism
- **MLM / ITM:** Aligning parts of images with text with masked-language modeling and image-text matching objectives
- **No Training:** Using stand-alone vision and language models via iterative optimization

History:
- 2022-12 [Vision-Language Pre-training: Basics, Recent Advances, and Future Trends](https://arxiv.org/abs/2210.09263)
- 2023-02 [A Dive into Vision-Language Models](https://huggingface.co/blog/vision_language_pretraining)
- 2024-04 [Vision Language Models Explained](https://huggingface.co/blog/vlms)

[Is CLIP still state of the art - or what other text encoder is used? : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/19d6h7w/is_clip_still_state_of_the_art_or_what_other_text/)